{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <center> **CIFAR-10 Manifold Learning Comparison** </center>\n",
        "<br><br>\n",
        "\n",
        "### <center> Author: Omar Gabr </center>"
      ],
      "metadata": {
        "id": "i8SN9vbeNY-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is the Objective of this Project?**\n",
        "\n",
        "This project aims to compare different manifold learning algorithms on the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), which consists of 60,000 color images of size 32x32x3 divided into 10 classes. Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. The goal is to project the data into 2D using various manifold learning algorithms, and color the data by the class in which it belongs.\n",
        "\n",
        "The manifold learning algorithms to be used include\n",
        "1. Principle Component Analysis\n",
        "2. Isomap Embedding\n",
        "3. Locally Linear Embedding\n",
        "4. Multi-Dimensional Scaling\n",
        "5. Spectral Embedding\n",
        "6. T-distributed Stochastic Neighbor Embedding\n",
        "\n",
        "Additionally, the UMAP algorithm will also be used. All these algorithms are available in the sklearn and keras library."
      ],
      "metadata": {
        "id": "NbokhL0qNZBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Why Was this Dataset Chosen?**"
      ],
      "metadata": {
        "id": "M2vxktRpNZDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CIFAR-10 dataset was chosen because it provides a challenging image classification problem with a large number of classes and color images. This dataset is different from the MNIST dataset in that it contains 10 classes of color images with a size of 32x32x3, which makes it a more realistic representation of real-world image data.\n",
        "\n",
        "The goal of this project is to evaluate the performance of different manifold learning algorithms on this dataset and compare their ability to capture the underlying structure of the data while preserving class information.\n",
        "\n",
        "This analysis will provide insights into which algorithm is better suited for this type of dataset and potentially improve the performance of image classification tasks."
      ],
      "metadata": {
        "id": "iiZmk9ywUf12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Test Split**\n",
        "\n",
        "### Importing Dataset into Training and Testing Sets"
      ],
      "metadata": {
        "id": "B8z4W8CoNZGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "# split into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# verify that training and testing sets have appropriate sizes\n",
        "print(f\"Training Set Shapes: ({X_train.shape}, {y_train.shape})\")\n",
        "print(f\"Testing Set Shapes: ({X_test.shape}, {y_test.shape})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47YPUPGmS1rO",
        "outputId": "84412f5d-1fdb-4ff0-8cac-48921f5e5c50"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Shapes: ((50000, 32, 32, 3), (50000, 1))\n",
            "Testing Set Shapes: ((10000, 32, 32, 3), (10000, 1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Manifold Learning Algorithms**"
      ],
      "metadata": {
        "id": "wtMK8gIZS4RC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a Data-Fitting Function for Manifold Learning Algorithms\n",
        "\n",
        "Ensuring the same scale is used over all features, because manifold learning methods are based on a nearest-neighbor search."
      ],
      "metadata": {
        "id": "-UHu5oD_XrQz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_8ygUDoS7V-",
        "outputId": "79f4e2cd-127a-4675-ca59-4d90fc11860f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <center> Principal Component Analysis (PCA) </center>\n",
        "\n",
        "PCA is the main linear algorithm for dimension reduction often used in unsupervised learning. It is a linear dimensionality reduction technique using singular value decomposition of the data to project it to a lower dimensional space. The input data is centered, but not scaled, for each feature before applying the SVD."
      ],
      "metadata": {
        "id": "dT5idVOrYI1b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "caIkQD6MYPqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <center> Isomap Embedding </center>\n",
        "\n",
        "Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points. Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA."
      ],
      "metadata": {
        "id": "6fwe7UsfYPwk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "odqiLS5BYP3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <center> Locally Linear Embedding (LLE) </center>\n",
        "\n",
        "LLE seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding."
      ],
      "metadata": {
        "id": "YAaAUA8rYP9J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uy9bEH5eYQDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <center> Multi-Dimensional Scaling (MDS) </center>\n",
        "\n",
        "MDS seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space. It is a technique used for analyzing similarity or dissimilarity data. It attempts to model such data as distances in a geometric spaces."
      ],
      "metadata": {
        "id": "rzRNvs6bYQJc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yACI2WL0YQOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <center> Spectral Embedding </center>\n",
        "\n",
        "Spectral Embedding is an approach to calculating a non-linear embedding. This method aims to find a simplified version of the data by using a mathematical technique called spectral decomposition of the graph Laplacian. This creates a graph that approximates the data's structure in a lower dimension.\n"
      ],
      "metadata": {
        "id": "oCi6wt2WYQTl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "khlIQchDYQYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <center> t-distributed Stochastic Neighbor Embedding (t-SNE) </center>\n",
        "\n",
        "t-SNE is a technique that helps visualize high-dimensional data by creating a low-dimensional representation. It works by converting the relationships between data points in the high-dimensional space into probabilities.\n",
        "\n",
        "In the original space, the relationships are depicted using Gaussian joint probabilities, while in the embedded space, they are illustrated using Student's t-distributions.\n"
      ],
      "metadata": {
        "id": "-LP0BeOlYQeB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9E5RHPBLYQjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparison Evaluation**"
      ],
      "metadata": {
        "id": "E4n8GmGrTNhs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1pEPgq_WTPHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "H5wobOCjUyQw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zL2dC-oqUz7C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}